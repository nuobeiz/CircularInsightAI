{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 50\n",
      "First example:\n",
      "{'role': 'system', 'content': \"You are a venture capital expert evaluating potential circular economy startup pitches. Mark the startup idea (problem and solution) from 1 to 3 in integer numbers (where 1 is bad, 2 is okay, and 3 is good) in each of four criteria: relevance of the problem to the circular economy (relevance_problem), clarity of the problem (clarity_problem), suitability of solution to the problem (suitability_solution) and clarity of the solution (clarity_solution). Return the following fields in a JSON dict: 'relevance_problem', 'clarity_problem', 'suitability_solution' and 'clarity_solution'.\"}\n",
      "{'role': 'user', 'content': \"{'problem': 'Plastic waste pollution', 'solution': 'recycling single-use plastic waste and converting it into interlocking tiles.'}\"}\n",
      "{'role': 'assistant', 'content': \"{'relevance_problem': 3, 'clarity_problem': 1, 'suitability_solution': 3, 'clarity_solution': 1}\"}\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../data/json/test_examples.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "# Get message\n",
    "message = dataset[index]['messages']\n",
    "system_content = message[0]['content']\n",
    "user_content = message[1]['content']\n",
    "assistant_content = message[2]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a venture capital expert evaluating potential circular economy startup pitches. Mark the startup idea (problem and solution) from 1 to 3 in integer numbers (where 1 is bad, 2 is okay, and 3 is good) in each of four criteria: relevance of the problem to the circular economy (relevance_problem), clarity of the problem (clarity_problem), suitability of solution to the problem (suitability_solution) and clarity of the solution (clarity_solution). Return the following fields in a JSON dict: 'relevance_problem', 'clarity_problem', 'suitability_solution' and 'clarity_solution'.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'problem': 'Plastic waste pollution', 'solution': 'recycling single-use plastic waste and converting it into interlocking tiles.'}\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'relevance_problem': 3, 'clarity_problem': 1, 'suitability_solution': 3, 'clarity_solution': 1}\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get response from OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get response from OpenAI API\n",
    "def get_response(system_content, user_content, model):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        response_format={ \"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "            ],\n",
    "        temperature=0,\n",
    "        seed=0\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_outputs(index):\n",
    "    # Get message\n",
    "    message = dataset[index]['messages']\n",
    "    system_content = message[0]['content']\n",
    "    user_content = message[1]['content']\n",
    "    assistant_content = message[2]['content']\n",
    "\n",
    "    assistant_content_default = get_response(system_content, user_content, model=\"gpt-3.5-turbo-1106\")\n",
    "    assistant_content_finetuned = get_response(system_content, user_content, model=\"ft:gpt-3.5-turbo-1106:personal::8e9YXb9p\")\n",
    "\n",
    "    print(\"--- Comparing outputs: ---\")\n",
    "    print(\"GPT response (ground truth): \", assistant_content)\n",
    "    print(\"GPT response (default model): \", assistant_content_default)\n",
    "    print(\"GPT response (finetuned model): \", assistant_content_finetuned)\n",
    "\n",
    "    return assistant_content, assistant_content_default, assistant_content_finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Comparing outputs: ---\n",
      "GPT response (ground truth):  {'relevance_problem': 2, 'clarity_problem': 1, 'suitability_solution': 2, 'clarity_solution': 1}\n",
      "GPT response (default model):  {'relevance_problem': 3, 'clarity_problem': 3, 'suitability_solution': 3, 'clarity_solution': 3}\n",
      "GPT response (finetuned model):  {'relevance_problem': 2.0, 'clarity_problem': 2.0, 'suitability_solution': 2.0, 'clarity_solution': 2.0}\n"
     ]
    }
   ],
   "source": [
    "assistant_content, assistant_content_default, assistant_content_finetuned = compare_outputs(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance_problem</th>\n",
       "      <th>clarity_problem</th>\n",
       "      <th>suitability_solution</th>\n",
       "      <th>clarity_solution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   relevance_problem  clarity_problem  suitability_solution  clarity_solution\n",
       "0                2.0              2.0                   2.0               2.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_list = []\n",
    "metrics_list.append(assistant_content_finetuned)\n",
    "pd.DataFrame(metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('relevance_problem', 2.0), ('clarity_problem', 2.0), ('suitability_solution', 2.0), ('clarity_solution', 2.0)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
